[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logger Package",
    "section": "",
    "text": "This package helps to import Hobo logger files into R. Currently supported are the Pendant temperature and temperature and light intensity loggers and the Temperature Pro V2. The package can be used to combine logger data from multiple deployments at different sites and filters the data using the time of deployment and retrieval. Basic plotting allows error checking.\nIn summary, an Excel table is used to store logger metadata as the file name, site name and depths as well as the time of deployment and retrieval. Each logger file is imported, cleaned, and merged to a standardised table.\nThe current version of the package is 0.1 and was published on 23.06.2023."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Logger Package",
    "section": "",
    "text": "This package helps to import Hobo logger files into R. Currently supported are the Pendant temperature and temperature and light intensity loggers and the Temperature Pro V2. The package can be used to combine logger data from multiple deployments at different sites and filters the data using the time of deployment and retrieval. Basic plotting allows error checking.\nIn summary, an Excel table is used to store logger metadata as the file name, site name and depths as well as the time of deployment and retrieval. Each logger file is imported, cleaned, and merged to a standardised table.\nThe current version of the package is 0.1 and was published on 23.06.2023."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Logger Package",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\n2.1 File Export\nThe .hobo files have to be exported as .csv using HOBOware. With the paid version HOBOware Pro it is possible to bulk export multiple .hobo files (Tools &gt; Bulk File Export). The export settings can be changed in the HOBOware settings in the “General” tab in “Export settings” and have to be as follows:\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Setting\n    \n  \n  \n    Export file type\nText (.txt or .csv)\n    Export table data column separator\nComma (,)\n    Include line number column\nNo\n    Include plot title in header\nNo\n    Always show fractional seconds\nNo\n    Separate date and time into two columns\nNo\n    No quotes or commas in headings, properties in parenthesis\nNo\n    Include logger serial number\nYes\n    Include sensor serial number or label if available\nYes\n    Date format\nY M D\n    Date seperator\nDash (-)\n    Time format\n24-Hour\n    Positive number format\n1,234.56\n    Negative number format\n-123\n    Include plot details in exported file\nNo\n  \n  \n  \n\n\n\n\nAdditionally, the computer language has to be English and HOBOware has to use SI units.\nI store the exported .csv files in the RStudio project folder under /data/raw_csv. I also keep a copy of the original .hobo files under /data/raw_hobo.\n\n\n2.2 Logger Metadata\nAn Excel table is used to store information about the logger, e.g. to which site it was deployed, at what depth, and when it was deployed and retrieved. Here is an example for such a table.\nNecessary columns are\n\nfilename: The file name of the .csv file. I add the date of the retrieval before the label that was set in HOBOware, e.g. 2023_06_01_Site1_Depth2.csv.\ndeployed: Date and time of the deployment of the logger. In Excel, use “Custom” format with type TT.MM.JJ hh:mm.\nretrieved: Date and time of the retrieval of the logger. The cell format should be as for deployed.\n\nThe other information is optional but adding the site name, depth, battery state, logged parameters, logging interval, etc. will help to keep track of the loggers. Additionally, I use the logger serial number (column SN). All additional columns can later be added to the logger data.\nI store this Excel file in the RStudio project folder under /data."
  },
  {
    "objectID": "index.html#install-package",
    "href": "index.html#install-package",
    "title": "Logger Package",
    "section": "3 Install Package",
    "text": "3 Install Package\nThis can be done with the install_github() function of the devtools package:\n\nlibrary(devtools)\ndevtools::install_github(\"andieich/Logger\")\n\nNow, the package can be loaded:\n\nlibrary(Logger)"
  },
  {
    "objectID": "index.html#use-package",
    "href": "index.html#use-package",
    "title": "Logger Package",
    "section": "4 Use Package",
    "text": "4 Use Package\n\n4.1 Summary\nIn short, the logger metadata from the Excel file is used to import all logger .csv files, filter the data by the time of deployment and retrieval and adds the additional metadata, e.g. site name, depth, etc. All imported files will be combined to a data.frame in the “long” format so it can be easily used with e.g. ggplot(). Finally, some ways to summarise and visualise the data are shown.\n\n\n4.2 Load packages\nThese additional packages will be used in this example:\n\nlibrary(tidyverse)  # for dplyr, ggplot, magrittr, etc.\nlibrary(lubridate)  # to facilitate date formatting\nlibrary(here)       # for relative file paths\n\n\n\n4.3 Read Logger Metadata\nThe Excel file containing information on the deployment and retrieval times of the logger has to be imported to R. To do so, read_loggerinfo() can be used.\n\nloggerinfo &lt;- read_loggerinfo(here::here(\"data/logger_info.xlsx\"))\n\n\n\n\n\n\n\n  \n    \n    \n      SN\n      type\n      site\n      depth\n      replicate\n      Batt%\n      BattV\n      intervall (h)\n      parameters\n      started\n      memory full\n      name/label\n      deployed\n      spot\n      retrieved\n      battery end\n      filename\n      redeployed\n      comment\n    \n  \n  \n    21086840\nPendant\nE2B\ndeep\n1\n0.83\n3.06\n1\nL, T\n2023-03-01 10:53:00\n2026-06-17 22:53:00\nE2B_deep_1\n2023-03-01 15:30:00\n2A\n2023-06-16 08:00:00\nNA\n2023_06_16_E2B_deep_1.csv\nno\nNA\n    21086844\nPendant\nE2B\nshallow\n2\n1\n3.08\n1\nL, T\n2023-03-16 12:53:00\n2026-07-03 00:53:00\nE2B_shallow_2\n2023-03-16 17:30:00\nrebar 2\n2023-06-16 08:00:00\nNA\n2023_06_16_E2B_shallow_2.csv\nno\nNA\n    21636685\nPro\nTemae\ndeep\nNA\ngood\nNA\n1\nT\n2023-03-08 13:10:00\n2028-03-06 13:10:00\nTemae_deep\n2023-03-08 15:30:00\n1A\n2023-06-19 08:00:00\ngood\n2023_06_19_Temae_deep.csv\nyes\nNA\n  \n  \n  \n\n\n\n\nThe data of all loggers in this table will be imported and merged in the next step. If you want, you can therefore filter loggerinfo, e.g. by site\n\nloggerinfo_sel1 &lt;- loggerinfo %&gt;% \n  dplyr::filter(site == \"E2B\")\n\n\n\n\n\n\n\n  \n    \n    \n      SN\n      type\n      site\n      depth\n      replicate\n      Batt%\n      BattV\n      intervall (h)\n      parameters\n      started\n      memory full\n      name/label\n      deployed\n      spot\n      retrieved\n      battery end\n      filename\n      redeployed\n      comment\n    \n  \n  \n    21086840\nPendant\nE2B\ndeep\n1\n0.83\n3.06\n1\nL, T\n2023-03-01 10:53:00\n2026-06-17 22:53:00\nE2B_deep_1\n2023-03-01 15:30:00\n2A\n2023-06-16 08:00:00\nNA\n2023_06_16_E2B_deep_1.csv\nno\nNA\n    21086844\nPendant\nE2B\nshallow\n2\n1\n3.08\n1\nL, T\n2023-03-16 12:53:00\n2026-07-03 00:53:00\nE2B_shallow_2\n2023-03-16 17:30:00\nrebar 2\n2023-06-16 08:00:00\nNA\n2023_06_16_E2B_shallow_2.csv\nno\nNA\n  \n  \n  \n\n\n\n\nor by the deployment time:\n\nloggerinfo_sel2 &lt;- loggerinfo %&gt;% \n  dplyr::filter(deployed &gt;= lubridate::as_date(\"2023-03-08\") )\n\n\n\n\n\n\n\n  \n    \n    \n      SN\n      type\n      site\n      depth\n      replicate\n      Batt%\n      BattV\n      intervall (h)\n      parameters\n      started\n      memory full\n      name/label\n      deployed\n      spot\n      retrieved\n      battery end\n      filename\n      redeployed\n      comment\n    \n  \n  \n    21086844\nPendant\nE2B\nshallow\n2\n1\n3.08\n1\nL, T\n2023-03-16 12:53:00\n2026-07-03 00:53:00\nE2B_shallow_2\n2023-03-16 17:30:00\nrebar 2\n2023-06-16 08:00:00\nNA\n2023_06_16_E2B_shallow_2.csv\nno\nNA\n    21636685\nPro\nTemae\ndeep\nNA\ngood\nNA\n1\nT\n2023-03-08 13:10:00\n2028-03-06 13:10:00\nTemae_deep\n2023-03-08 15:30:00\n1A\n2023-06-19 08:00:00\ngood\n2023_06_19_Temae_deep.csv\nyes\nNA\n    21636679\nPro\nTemae\nshallow\nNA\ngood\nNA\n1\nT\n2023-03-08 13:10:00\n2028-03-06 13:10:00\nTemae_shallow\n2023-03-08 17:30:00\n1A\n2023-06-19 08:00:00\ngood\n2023_06_19_Temae_shallow.csv\nno\nNA\n  \n  \n  \n\n\n\n\n\n\n4.4 Import logger .csv files\nNow, the actual data can be imported. To do so, loggerinfo, that was just imported, has to be provided to the read_loggerfiles() function. During import, it is displayed how many data columns are removed due to filtering by the deployment and retrial time.\n\nloggerdata &lt;- read_loggerfiles(loggerinfo)\n\nℹ 2023_06_16_E2B_deep_1.csv\n• removed 58 of 2619 rows (2.2%)\n✔ Read 1 of 4 files.\n\n\nℹ 2023_06_16_E2B_shallow_2.csv\n• removed 58 of 2257 rows (2.6%)\n✔ Read 2 of 4 files.\n\n\nℹ 2023_06_19_Temae_deep.csv\n• removed 7 of 2471 rows (0.3%)\n✔ Read 3 of 4 files.\n\n\nℹ 2023_06_19_Temae_shallow.csv\n• removed 7 of 2469 rows (0.3%)\n✔ Read 4 of 4 files.\n\n\n\n\n\n\n\n\n  \n    \n    \n      date_time\n      temperature\n      light_intensity\n      SN\n      type\n      site\n      depth\n      filename\n    \n  \n  \n    2023-03-01 15:53:26\n28.953\n258.3\n21086840\nPendant\nE2B\ndeep\n2023_06_16_E2B_deep_1.csv\n    2023-03-01 16:53:26\n29.053\n204.5\n21086840\nPendant\nE2B\ndeep\n2023_06_16_E2B_deep_1.csv\n    2023-03-01 17:53:26\n29.053\n172.2\n21086840\nPendant\nE2B\ndeep\n2023_06_16_E2B_deep_1.csv\n    2023-03-01 18:53:26\n29.053\n0.0\n21086840\nPendant\nE2B\ndeep\n2023_06_16_E2B_deep_1.csv\n    2023-03-01 19:53:26\n29.053\n0.0\n21086840\nPendant\nE2B\ndeep\n2023_06_16_E2B_deep_1.csv\n  \n  \n  \n\n\n\n\nAs a check, the function compares the serial number (SN) provided in loggerinfo against the SN stored in the logger .csv file. If this is not desired, e.g. because SN is not stored in loggerinfo, you can turn this check off:\n\nread_loggerfiles(loggerinfo, check_SN = FALSE)\n\nIf your logger .csv files are not stored in /data/raw_csv, you have to specify this folder:\n\nread_loggerfiles(loggerinfo, folder = \"your_folder\")\n\n\n\n4.5 Check data\nNow the data has to be checked. This can be useful to detect if the deployment and retrial times are actually correct (check for sudden changes).\nAs a default, the temperature is plotted.\n\nplot_all_deployments(loggerdata)\n\n\n\n\nThe plotted parameter can be changed to light_intensity:\n\nplot_all_deployments(loggerdata, parameter = \"light_intensity\")\n\n\n\n\nNext, the data can be summarised to the daily mean, minimum, and maximum values. This can be done with the summarise_daily() function. It can summarise the temperature and light_intensity data. As a default, the data is grouped by filename and date. Additional grouping can be done by adding columns to by.\nDefault settings:\n\nloggerdata_tempS &lt;- summarise_daily(loggerdata)\n\n`summarise()` has grouped output by 'date'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n  \n    \n    \n      date\n      filename\n      mean_temperature\n      min_temperature\n      max_temperature\n    \n  \n  \n    2023-03-01\n2023_06_16_E2B_deep_1.csv\n29.05289\n28.953\n29.152\n    2023-03-02\n2023_06_16_E2B_deep_1.csv\n28.99508\n28.060\n29.552\n    2023-03-03\n2023_06_16_E2B_deep_1.csv\n29.03208\n28.754\n29.252\n    2023-03-04\n2023_06_16_E2B_deep_1.csv\n29.11083\n28.853\n29.452\n    2023-03-05\n2023_06_16_E2B_deep_1.csv\n29.04850\n28.555\n29.252\n  \n  \n  \n\n\n\n\nTo include additional groups, use by, e.g.:\n\nloggerdata_tempS &lt;- summarise_daily(loggerdata, \n                                    by = c(\"site\", \"depth\", \"type\", \"SN\"))\n\n`summarise()` has grouped output by 'date', 'site', 'depth', 'type', 'SN'. You\ncan override using the `.groups` argument.\n\n\n\n\n\n\n\n\n  \n    \n    \n      date\n      site\n      depth\n      type\n      SN\n      filename\n      mean_temperature\n      min_temperature\n      max_temperature\n    \n  \n  \n    2023-03-01\nE2B\ndeep\nPendant\n21086840\n2023_06_16_E2B_deep_1.csv\n29.05289\n28.953\n29.152\n    2023-03-02\nE2B\ndeep\nPendant\n21086840\n2023_06_16_E2B_deep_1.csv\n28.99508\n28.060\n29.552\n    2023-03-03\nE2B\ndeep\nPendant\n21086840\n2023_06_16_E2B_deep_1.csv\n29.03208\n28.754\n29.252\n    2023-03-04\nE2B\ndeep\nPendant\n21086840\n2023_06_16_E2B_deep_1.csv\n29.11083\n28.853\n29.452\n    2023-03-05\nE2B\ndeep\nPendant\n21086840\n2023_06_16_E2B_deep_1.csv\n29.04850\n28.555\n29.252\n  \n  \n  \n\n\n\n\nSimilarly, the light_intensty can be summarised:\n\nloggerdata_lightS &lt;- summarise_daily(loggerdata, \n                                     by = c(\"site\", \"depth\", \"type\", \"SN\"), \n                                     parameter = \"light_intensity\")\n\n`summarise()` has grouped output by 'date', 'site', 'depth', 'type', 'SN'. You\ncan override using the `.groups` argument.\n\n\nThe data can be plotted like this:\n\nloggerdata_tempS %&gt;% \n  ggplot(aes(x = date))+\n  geom_ribbon(aes(ymin = min_temperature,\n                  ymax = max_temperature,\n                  group = filename), \n              alpha = .3)+\n  geom_line(aes(y = mean_temperature,\n                group = filename), \n            linewidth = .1)+\n  labs(x = NULL, y = \"Temperature (°C)\")+\n  scale_x_date(date_labels = \"%d.%m.%y\")+\n  facet_grid(site ~ depth)+\n  theme_minimal()"
  }
]